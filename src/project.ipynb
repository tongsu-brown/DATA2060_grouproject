{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Overview of Gaussian Naive Bayes for classification**"
      ],
      "metadata": {
        "id": "f8vuqtpuZc44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 overview of the algorithm:**\n",
        "\n",
        "**1.1.1 Description:**\n",
        "\n",
        "Gaussian Naive Bayes is a generative model for classification. It applies the similar assumpotion as Naive Bayes that the features are conditionally independent. [1,2](#ref-1,2)\n",
        "\n",
        "Instead of limiting features to be discrete(categorical, binary) in Naive Bayes, the features in Gaussian Naive Bayes can be continous. We first assume the probabliity of lables are same as $P[Y=1]=P[Y=0] = 1/2$. Then the conditional likelihood (probability of $X$ given $Y$) is a Gaussian distribution:\n",
        "\n",
        "$$P(x_i|y)=\\frac{1}{\\sqrt{(2\\pi\\sigma_y^2)}}exp(-\\frac{(x_i-μ_y)^2}{2σ_y^2})$$\n",
        "\n",
        "where the variance $σ_y$ and mean $μ_y$ for class $y$ are estiamted by the MLE (maximum likelihood estimator):\n",
        "$$h_{Bayes}(x) = \\mathrm{argmax}_{y\\in\\{0,1\\}}[\\mathrm{log}(p(y))+Σ_i\\mathrm{log}p(x_i|y)] $$\n",
        "\n",
        "\n",
        "\n",
        "**1.1.2 Advantages:**\n",
        "\n",
        "*   efficient and powerful: have competitive performance even if the conditional independence is voided [3](#ref-3)\n",
        "*   fast and stable: MLE and trained in lower space but works well for higher dimension\n",
        "*   in-situ update: support online updates to parameters via [partial fit](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?utm_source=chatgpt.com#sklearn.naive_bayes.GaussianNB.partial_fit)\n",
        "\n",
        "\n",
        "**1.1.3 Disadvantage:**\n",
        "\n",
        "*   Assume features to be conditional independent, but not always true\n",
        "*   Assume Gaussian distribution, but features can have other distribution like delta, even or just random\n",
        "\n"
      ],
      "metadata": {
        "id": "hehf28oIaRGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Representation (Tong):**\n",
        "\n",
        "describe how the feature values are converted into a single number prediction.\n",
        "\n",
        "For $d$ continous features $X \\in \\mathbb{R}^d$,   $x = (x_1,...,x_d)$\n",
        "\n",
        "Class $y$ can be binary $y \\in \\{0,1\\}$ or $K$ multiclass $y \\in \\{0,1,...K-1\\}$\n",
        "\n",
        "The conditionally independent likelihood can be represented by:\n",
        "$$\n",
        "p(x\\mid y) = \\prod_{i=1}^d \\mathcal{N}(x_i \\mid μ_{iy}, σ_{iy}^2) = \\prod_{i=1}^d\\frac{1}{\\sqrt{(2\\pi\\sigma_{iy}^2)}}exp(-\\frac{(x_i-μ_{iy})^2}{2σ_{iy}^2})\n",
        "$$\n",
        "where $μ = \\mathbb{E}[x] \\in \\mathbb{R}^d $ and $σ^2 \\in \\mathbb{R}_+^{\\,d}$\n",
        "\n",
        "If we use covariance matrix $Σ = \\mathrm{cov}(x) = \\mathrm{Var}(x) \\in \\mathbb{R}^{d\\times d}$, the conditional likelihood can be written by:\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}\\mid y)\n",
        "= \\mathcal{N}(\\mathbf{x}\\mid \\boldsymbol{\\mu}_y,\\boldsymbol{\\Sigma}_y)\n",
        "= \\frac{1}{(2\\pi)^{d/2}\\,|\\boldsymbol{\\Sigma}_y|^{1/2}}\n",
        "\\exp\\!\\left(\n",
        "-\\tfrac12 (\\mathbf{x}-\\boldsymbol{\\mu}_y)^{\\top}\\boldsymbol{\\Sigma}_y^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_y)\n",
        "\\right).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "FfJIDdQ6Zc8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Loss (Luo):**\n",
        "\n",
        "Gaussian Naive Bayes uses Maximum Likelihood Estimation (MLE) to train the model. Unlike discriminative models that directly minimize prediction error, GNB takes a generative approach by modeling the joint probability distribution of features and labels. The objective is to find a set of parameters $\\theta = (\\mu, \\sigma^2, P(y))$, that maximizes the probability (likelihood) $L(\\theta | D)$ of the training data $D$ occurring.\n",
        "Given a training dataset $D = \\{(x^{(j)}, y^{(j)})\\}_{j=1}^n$ where $x^{(j)} \\in \\mathbb{R}^d$ and\n",
        "$y^{(j)} \\in \\{0, 1, \\ldots, K-1\\}$, the likelihood function is:\n",
        "\n",
        "$$\n",
        "L(\\theta \\mid D)\n",
        "= \\prod_{j=1}^n P\\big(y^{(j)}, x^{(j)}\\big)\n",
        "= \\prod_{j=1}^n P\\big(y^{(j)}\\big)\\prod_{i=1}^d P\\big(x_i^{(j)} \\mid y^{(j)}\\big)\n",
        "$$\n",
        "\n",
        "\n",
        "And numerical stability and easier computation, we typically maximize the log-likelihood instead, which converts the product into a sum:\n",
        "$$  \n",
        "\\ell(\\theta | D) = \\sum_{(\\mathbf{x}^{(j)}, y^{(j)}) \\in D} \\left( \\log P(y^{(j)}) + \\sum_{i=1}^{d} \\log P(x_i^{(j)} | y^{(j)}) \\right)\n",
        "$$\n",
        "Due to the naive assumption, the parameters for $\\log P(y^{(j)})$ and the parameters for each $\\log P(x_i^{(j)} | y^{(j)})$ (i.e., $\\mu_{k,i}, \\sigma_{k,i}^2$) can be optimized independently. <br>\n",
        "This transformation is valid because the logarithm is a monotonically increasing function, so maximizing the log-likelihood is equivalent to maximizing the original likelihood. Due to the naive assumption of conditional independence among features, the parameters can be optimized independently. The class prior parameters depend only on the class label distribution, while for each class and feature, the mean and variance parameters depend only on the samples belonging to that class.<br>\n",
        "While MLE maximizes likelihood, we can equivalently view this as minimizing the negative log-likelihood, which aligns with the typical machine learning framework of minimizing a loss function. The use of MLE as the training objective means there are no hyperparameters to tune, and the optimal parameters can be computed directly through closed-form solutions rather than iterative optimization. However, without regularization, MLE can overfit when training data is limited, particularly when some features have very small variance within certain classes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CVIn3EHcZc_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Optimizer:**\n",
        "\n",
        "For Gaussian Navie Bayes, training is interpreted by maximizing the log likelihood(MLE). So it is a closed-form solution without the interative numerical optimizer."
      ],
      "metadata": {
        "id": "knVJykvQZdCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.5 Algorithm Pseudocode:**\n",
        "**1.5.1 Train**\n",
        "**Input:**  \n",
        "- `X_train`: training data, shape (n_samples, n_features)  \n",
        "- `y_train`: training labels, length n_samples  \n",
        "\n",
        "\n",
        "1. Get basic information  \n",
        "   - `classes ← unique(y_train)`  \n",
        "   - `n_classes ← length(classes)`  \n",
        "   - `n_features ← number of columns in X_train`  \n",
        "2. Initialize model parameters:μ, σ², priors\n",
        "3. FOR `idx` from `0` to `n_classes − 1`:\n",
        "   1. Let `c ← classes[idx]`  \n",
        "   2. Select samples of class `c`:  \n",
        "      - `X_c ← rows of X_train where y_train == c`  \n",
        "   3. Compute MLE estimates for this class:  \n",
        "      - `μ[idx, :] ← mean of X_c over rows`  \n",
        "      - `σ²[idx, :] ← variance of X_c over rows`  \n",
        "   4. Compute class prior:  \n",
        "      - `priors[idx] ← (number of rows in X_c) / (number of rows in X_train)`  \n",
        "4. Apply variance smoothing for numerical stability:  \n",
        "   - `ε ← var_smoothing × max(variance(X_train))`  \n",
        "   - `σ² ← σ² + ε`  \n",
        "\n",
        "**1.5.2 Joint Log-Likelihood Computation**\n",
        "**Input:**  \n",
        "- `X_input`: data to evaluate, shape (n_samples, n_features)  \n",
        "\n",
        "1. Let `n_samples ← number of rows in X_input`  \n",
        "2. Initialize:  \n",
        "   - `joint_log_likelihood ← zeros(n_samples, n_classes)`  \n",
        "\n",
        "3. FOR `idx` from `0` to `n_classes − 1`:\n",
        "   1. Compute log prior for this class:  \n",
        "      - `log_prior ← log(priors[idx])`  \n",
        "   2. For all samples in `X_input`, compute Gaussian log-likelihood under class `idx`  \n",
        "      - Using `μ[idx, :]` and `σ²[idx, :]` for the Gaussian density  \n",
        "      - `log_likelihood ← sum over all features of log Gaussian density`  \n",
        "   3. Combine prior and likelihood:  \n",
        "      - `joint_log_likelihood[:, idx] ← log_prior + log_likelihood`  \n",
        "\n",
        "4. Return `joint_log_likelihood`.  \n",
        "\n",
        "**1.5.3 Prediction**\n",
        "**Input:**  \n",
        "- `X_input`: test data, shape (n_samples, n_features)  \n",
        "\n",
        "1. Compute joint log-likelihoods for all samples:  \n",
        "   - `joint_log_likelihood ← JointLogLikelihood(X_input)`  \n",
        "\n",
        "2. Initialize prediction array `y_pred` of length `n_samples`.  \n",
        "\n",
        "3. FOR `i` from `0` to `n_samples − 1`:\n",
        "   1. Find index of maximum log-likelihood:  \n",
        "      - `class_idx ← argmax over classes of joint_log_likelihood[i, :]`  \n",
        "   2. Map index back to class label:  \n",
        "      - `y_pred[i] ← classes[class_idx]`  \n",
        "\n",
        "4. Return `y_pred` as the predicted labels for `X_input`."
      ],
      "metadata": {
        "id": "yS0ritRJNBqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "<a id=\"ref-1\"></a>[1] scikit-learn User Guide — Naive Bayes (Gaussian formula, MLE parameters, practical notes). [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html?utm_source=chatgpt.com#gaussian-naive-bayes)\n",
        "\n",
        "<a id=\"ref-2\"></a>[2] Shalev-Shwartz S, Ben-David S. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press; 2014.[brown-access-only](https://www-cambridge-org.revproxy.brown.edu/core/books/understanding-machine-learning/3059695661405D25673058E43C8BE2A6)\n",
        "\n",
        "<a id=\"ref-3\"></a>[3] Zhang, Harry. \"The optimality of naive Bayes.\" Aa 1.2 (2004): 3. [Zhang](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)\n"
      ],
      "metadata": {
        "id": "T-5KJSvyWIbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Model**"
      ],
      "metadata": {
        "id": "mCZ9f90TZlpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class GaussianNaiveBayes(object):\n",
        "\n",
        "    \"\"\" Gaussian Naive Bayes model\n",
        "\n",
        "    @attrs:\n",
        "        n_classes:    the number of classes, as defined by object\n",
        "        #attr_dist:    a 2D (n_classes x n_attributes) NumPy array of the attribute distributions\n",
        "        label_priors: a 1D NumPy array of the priors distribution\n",
        "        mu:           a 2D (n_classes x n_attributes) NumPy array of the means for each class\n",
        "        var:        a 2D (n_classes x n_attributes) NumPy array of the variance for each class\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, var_smoothing = 1e-9):\n",
        "\n",
        "        \"\"\" Initialize a Gaussian Naive Bayes model with n_classes\"\"\"\n",
        "\n",
        "        self.classes      = None\n",
        "        self.n_classes    = None\n",
        "        #self.attr_dist    = None\n",
        "        self.label_priors = None\n",
        "        self.mu           = None\n",
        "        self.var        = None\n",
        "        self.var_smoothing = var_smoothing\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "\n",
        "        \"\"\" Trains the model, using maximum likelihood estimation.\n",
        "        @params:\n",
        "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
        "            y_train: a 1D (n_examples) numpy array\n",
        "        @return:\n",
        "            a tuple consisting of:\n",
        "                1) mu:    a 2D (n_classes x n_attributes) NumPy array of the means for each class\n",
        "                2) var: a 2D (n_classes x n_attributes) NumPy array of the variances for each class\n",
        "                3) label_priors: a 1D NumPy array of the priors distribution\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(X_train, pd.DataFrame):\n",
        "            X_train = X_train.values\n",
        "        if isinstance(y_train, pd.Series):\n",
        "            y_train = y_train.values\n",
        "\n",
        "        n_features = X_train.shape[1]\n",
        "\n",
        "        self.classes = np.unique(y_train) #np unique classes\n",
        "        self.n_classes = len(self.classes)\n",
        "        self.label_priors = np.zeros(self.n_classes)\n",
        "        #self.attr_dist    = np.zeros((self.n_classes, X_train.shape[1]))\n",
        "        self.mu           = np.zeros((self.n_classes, n_features)) #不必重复调用X_train.shape，使用n_features 代替\n",
        "        self.var        = np.zeros((self.n_classes, n_features))\n",
        "\n",
        "        for idx in range(self.n_classes):\n",
        "            c=self.classes[idx]\n",
        "            X_c = X_train[y_train == c]\n",
        "\n",
        "            '''\n",
        "            # Update: check for empty class\n",
        "            if X_c.shape[0] == 0:\n",
        "                raise ValueError(f\"Class {c} has no training samples. All classes must have at least one sample.\")\n",
        "                ''' #使用unique的情况下，不需要test empty class\n",
        "\n",
        "\n",
        "            #self.mu = np.mean(X_c, axis=0)  # 每次循环都被覆盖了吧？\n",
        "            #self.sigma = np.std(X_c, axis=0)  # ^\n",
        "            self.mu[idx, :] = np.mean(X_c, axis=0) # 已修改成对应的类\n",
        "            self.var[idx, :] = np.var(X_c, axis=0)\n",
        "            self.label_priors[idx] = X_c.shape[0] / X_train.shape[0]\n",
        "\n",
        "        epslion = self.var_smoothing * np.var(X_train, axis=0).max()\n",
        "        self.var += epslion\n",
        "\n",
        "\n",
        "    def _joint_log_likelihood(self, X_input):\n",
        "\n",
        "        \"\"\" Computes the log likelihood\n",
        "        @params:\n",
        "            X_input: a 2D (n_examples x n_attributes) numpy array\n",
        "        @return:\n",
        "            joint_log_likelihood: a 2D (n_examples x n_classes) numpy array\n",
        "            where joint_log_likelihood[i, c] = log P(y=c) + sum_j log P(x_j | y=c)\n",
        "        \"\"\"\n",
        "        n_samples = X_input.shape[0]\n",
        "        joint_log_likelihood = np.zeros((n_samples, self.n_classes))\n",
        "\n",
        "        for idx in range(self.n_classes):\n",
        "            prior = np.log(self.label_priors[idx])\n",
        "            log_likelihood = -0.5 * np.sum(np.log(2.0 * np.pi * self.var[idx, :]))\n",
        "\n",
        "            log_likelihood -= np.sum(((X_input - self.mu[idx, :]) ** 2) / (2 * self.var[idx, :] ), axis=1)\n",
        "            joint_log_likelihood[:, idx] = prior + log_likelihood\n",
        "\n",
        "        return joint_log_likelihood\n",
        "\n",
        "\n",
        "    def predict(self, X_input):\n",
        "\n",
        "        \"\"\" Outputs predictions of the input\n",
        "        \"\"\"\n",
        "        joint_log_likelihood = self._joint_log_likelihood(X_input)\n",
        "\n",
        "        pred_idx = np.argmax(joint_log_likelihood, axis=1)\n",
        "        y_pred = self.classes[pred_idx]  # return classes\n",
        "        return y_pred\n",
        "\n",
        "    def accuracy(self, X_test, y_test):\n",
        "\n",
        "        \"\"\" Outputs the accuracy of the trained model on a given dataset (data).\n",
        "\n",
        "        @params:\n",
        "            X_test: a 2D numpy array of examples\n",
        "            y_test: a 1D numpy array of labels\n",
        "        @return:\n",
        "            a float number indicating accuracy (between 0 and 1)\n",
        "        \"\"\"\n",
        "        predictions = self.predict(X_test)\n",
        "        accuracy = np.mean(predictions == y_test)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "D7uSUzsehYVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Check Model**"
      ],
      "metadata": {
        "id": "E64VcNP4Z9ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "check model with pesudo data\n",
        "\"\"\"\n",
        "\n",
        "# import packages\n",
        "import pytest\n",
        "import numpy as np\n",
        "np.random.seed(2025)\n",
        "\n",
        "# Creates Test Models with 2 & 3 classes\n",
        "test_model1 = GaussianNaiveBayes()\n",
        "test_model2 = GaussianNaiveBayes()\n",
        "test_model3 = GaussianNaiveBayes()\n",
        "\n",
        "#Create a function for testing\n",
        "\n",
        "def check_train_dtype(model, mu, sigma, priors, x_train, y_train):\n",
        "    assert isinstance(mu, np.ndarray), f\"mu is not a numpy array\"\n",
        "    assert mu.ndim==2, f\"mu should be 2D\"\n",
        "    assert mu.shape==(model.n_classes, x_train.shape[1]), f\"mu shape should be (n_classes, n_features), got {mu.shape}\"\n",
        "    assert isinstance(sigma, np.ndarray), f\"sigma is not a numpy array\"\n",
        "    assert sigma.ndim==2, f\"sigma should be 2D\"\n",
        "    assert sigma.shape==(model.n_classes, x_train.shape[1]), f\"sigma shape should be (n_classes, n_features), got {sigma.shape}\"\n",
        "    assert isinstance(priors, np.ndarray), f\"priors is not a numpy array\"\n",
        "    assert priors.ndim==1, f\"priors should be 1D\"\n",
        "    assert priors.shape==(model.n_classes, ), f\"priors shape should be (n_classes), got {priors.shape}\"\n",
        "\n",
        "# Create and Test Data\n",
        "# 1D data test\n",
        "x1 = np.array([[0], [1], [2], [8], [9], [10]])\n",
        "y1 = np.array([0,0,0,1,1,1])\n",
        "x_test1 = np.array([[0.1],[9.2],[5.1]])\n",
        "y_test1 = np.array([0,1,1])\n",
        "y_test1_noise = np.array([0,0,1])\n",
        "exp_mu1 = np.array([[1],[9]])\n",
        "exp_var1 = np.array([[2/3],[2/3]])\n",
        "exp_priors1 = np.array([0.5,0.5])\n",
        "\n",
        "# train/check the model with 1D data set\n",
        "test_model1.train(x1,y1)\n",
        "mu1 = test_model1.mu\n",
        "var1 = test_model1.var\n",
        "priors1 = test_model1.label_priors\n",
        "check_train_dtype(test_model1, mu1, var1, priors1, x1, y1)\n",
        "assert (mu1 == pytest.approx(exp_mu1, 0.01))\n",
        "assert (var1 == pytest.approx(exp_var1, 0.01))\n",
        "assert (priors1 == pytest.approx(exp_priors1, 0.01))\n",
        "assert (test_model1.predict(x_test1) == pytest.approx(y_test1, 0.01))\n",
        "assert test_model1.accuracy(x_test1, y_test1) == 1\n",
        "assert test_model1.accuracy(x_test1, y_test1_noise) == 2/3\n",
        "\n",
        "# 2D data test\n",
        "x2 = np.array([[-1,-1],[-2,-1.6],[-4,-3],[4,4],[3,3],[2.8,6.4]])\n",
        "y2 = np.array([0,0,0,1,1,1])\n",
        "x_test2 = np.array([[0,0],[-6,-5],[4.2,3.9],[2.5,2.5]])\n",
        "y_test2 = np.array([0,0,1,1])\n",
        "exp_mu2 = np.array([[-2.333, -1.867],[3.267,4.467]])\n",
        "exp_var2 = np.array([[1.555,0.702],[0.276,2.036]])  # 方差\n",
        "exp_priors2 = np.array([0.5,0.5])\n",
        "\n",
        "# train/check the model with 2D data set\n",
        "\n",
        "test_model2.train(x2,y2)\n",
        "mu2 = test_model2.mu\n",
        "var2 = test_model2.var\n",
        "priors2 = test_model2.label_priors\n",
        "check_train_dtype(test_model2, mu2, var2, priors2, x2, y2)\n",
        "assert (mu2 == pytest.approx(exp_mu2, 0.01))\n",
        "assert (var2 == pytest.approx(exp_var2, 0.01))\n",
        "assert (priors2 == pytest.approx(exp_priors2, 0.01))\n",
        "assert (test_model2.predict(x_test2) == pytest.approx(y_test2, 0.01))\n",
        "assert test_model2.accuracy(x_test2, y_test2) == 1\n",
        "\n",
        "# 3 classes data test\n",
        "x3 = np.array([[0,0],[1,1],[2,2],[4,0],[5,1],[6,2],[0,4],[1,5],[2,6]])\n",
        "y3 = np.array([0,0,0,1,1,1,2,2,2])\n",
        "x_test3 = np.array([[0.5,0.3],[1.5,1.2],[4.2,0.2],[5.8,1.7],[0.2,4.1],[1.8,5.7]])\n",
        "y_test3 = np.array([0,0,1,1,2,2])\n",
        "y_test3_noise = np.array([0,0,0,1,1,2])\n",
        "exp_mu3 = np.array([[1,1],[5,1],[1,5]])\n",
        "exp_var3 = np.array([[2/3,2/3],[2/3,2/3],[2/3,2/3]])\n",
        "exp_priors3 = np.array([0.333,0.333,0.333])\n",
        "\n",
        "# Edge case 1: constant feature (zero variance)\n",
        "x_const = np.array([[1, 5], [1, 6], [1, 7], [2, 1], [2, 2], [2, 3]])\n",
        "y_const = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# Edge case 2: single feature\n",
        "x_single = np.array([[1], [2], [3], [10], [11], [12]])\n",
        "y_single = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# Edge case 3: empty class data\n",
        "x_empty = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y_empty = np.array([0, 0, 1])  # No samples for class 2\n",
        "\n",
        "# train/check model with 3 classes data set\n",
        "test_model3.train(x3,y3)\n",
        "mu3 = test_model3.mu\n",
        "var3 = test_model3.var\n",
        "priors3 = test_model3.label_priors\n",
        "check_train_dtype(test_model3, mu3, var3, priors3, x3, y3)\n",
        "assert (mu3 == pytest.approx(exp_mu3, 0.01))\n",
        "assert (var3 == pytest.approx(exp_var3, 0.01))\n",
        "assert (priors3 == pytest.approx(exp_priors3, 0.01))\n",
        "assert (test_model3.predict(x_test3) == pytest.approx(y_test3, 0.01))\n",
        "assert test_model3.accuracy(x_test3, y_test3) == 1\n",
        "assert test_model3.accuracy(x_test3, y_test3_noise) == 2/3\n",
        "\n",
        "print(\"3 classes data test passed!\")\n",
        "\n",
        "# EC1\n",
        "test_const = GaussianNaiveBayes()\n",
        "test_const.train(x_const, y_const)\n",
        "mu_const = test_const.mu\n",
        "var_const = test_const.var\n",
        "assert np.all(var_const[:, 0] > 0), \"Constant feature should have small non-zero variance\"\n",
        "assert var_const[0, 0] < 0.01, \"Constant feature sigma should be very small\"\n",
        "\n",
        "# EC2\n",
        "test_single = GaussianNaiveBayes()\n",
        "test_single.train(x_single, y_single)\n",
        "mu_single = test_single.mu\n",
        "var_single = test_single.var\n",
        "priors_single = test_single.label_priors\n",
        "assert mu_single.shape == (2, 1), f\"Expected mu shape (2, 1), got {mu_single.shape}\"\n",
        "assert var_single.shape == (2, 1), f\"Expected variance shape (2, 1), got {var_single.shape}\"\n",
        "predictions_single = test_single.predict(np.array([[2], [11]]))\n",
        "assert predictions_single[0] == 0 and predictions_single[1] == 1, \"Single feature predictions should be correct\"\n",
        "\n",
        "# EC3\n",
        "'''\n",
        "test_empty = GaussianNaiveBayes()\n",
        "try:\n",
        "    test_empty.train(x_empty, y_empty)\n",
        "    print(\"  ✗ Should have raised ValueError for empty class\")\n",
        "    assert False, \"Expected ValueError for empty class\"\n",
        "except ValueError as e:\n",
        "    assert \"Class 2 has no training samples\" in str(e), f\"Wrong error message: {e}\"\n",
        "'''\n",
        "\n",
        "# _joint_log_likelihood\n",
        "jll = test_model1._joint_log_likelihood(x_test1)\n",
        "assert jll.shape == (3, 2), f\"Expected jll shape (3, 2), got {jll.shape}\"\n",
        "assert np.all(jll < 0), \"All log likelihoods should be negative\"\n",
        "assert jll[0, 0] > jll[0, 1], \"Sample close to class 0 should prefer class 0\"\n",
        "assert jll[1, 1] > jll[1, 0], \"Sample close to class 1 should prefer class 1\"\n",
        "\n",
        "# predict()\n",
        "x_outlier = np.array([[100], [-100]])\n",
        "predictions_outlier = test_model1.predict(x_outlier)\n",
        "assert predictions_outlier.shape == (2,), \"Predictions should have correct shape\"\n",
        "assert predictions_outlier[0] == 1, \"Outlier at 100 should predict class 1 (closer to 9)\"\n",
        "assert predictions_outlier[1] == 0, \"Outlier at -100 should predict class 0 (closer to 1)\"\n",
        "\n",
        "# accuracy()\n",
        "acc_perfect = test_model2.accuracy(x_test2, y_test2)\n",
        "assert acc_perfect == 1.0, f\"Perfect accuracy should be 1.0, got {acc_perfect}\"\n",
        "\n",
        "# Test with all wrong predictions\n",
        "y_all_wrong = np.array([1, 1, 0, 0])\n",
        "acc_zero = test_model2.accuracy(x_test2, y_all_wrong)\n",
        "assert acc_zero == 0.0, f\"All-wrong accuracy should be 0.0, got {acc_zero}\"\n",
        "\n",
        "print(\"All edge case tests passed!\")\n"
      ],
      "metadata": {
        "id": "dLGHoXGCaBaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e78f19-c4c5-4094-a43f-4201b85f1ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 classes data test passed!\n",
            "All edge case tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Compare with sklearn\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PEwx_scOaBHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "\n",
        "# fetch dataset\n",
        "wine_quality = fetch_ucirepo(id=186)\n",
        "\n",
        "# full original data\n",
        "X = wine_quality.data.original.drop(columns=[\"quality\"])\n",
        "y = pd.Categorical(wine_quality.data.original[\"quality\"])\n",
        "\n",
        "# Number of samples\n",
        "print(f\"There are {len(X)} samples in the dataset\")\n",
        "\n",
        "# separate datasets by wine color\n",
        "X_red = X[X['color'] == 'red'].drop(columns=['color'])\n",
        "X_white = X[X['color'] == 'white'].drop(columns=['color'])\n",
        "\n",
        "y_red = y[X['color'] == 'red']\n",
        "y_white = y[X['color'] == 'white']\n",
        "\n",
        "print(f\"There are {len(X_red)} samples in the red wine dataset\")\n",
        "print(f\"There are {len(X_white)} samples in the white wine dataset\")"
      ],
      "metadata": {
        "id": "volMdphWmKzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f547bf-cc19-43ec-ad58-7cbd15e1fd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 6497 samples in the dataset\n",
            "There are 1599 samples in the red wine dataset\n",
            "There are 4898 samples in the white wine dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description:**\n",
        "\n",
        "The dataset used in this analysis is the Wine Quality dataset from the UCI Machine Learning Repository. The target variable is *quality*, representing the sensory quality score of each wine sample. The dataset contains 11 continuous features, as well as one categorical attribute, *color*, which indicates whether a sample is red or white wine.\n",
        "\n",
        "For consistency, we split the dataset into two separate subsets—one for red wine and one for white wine—based on the color attribute. This separation prevents potential distributional shifts or confounding effects that may arise when combining data originating from two different wine types with inherently distinct feature distributions. It also ensures that all features within each subset are continuous, which is important for validating our implementation of Gaussian Naive Bayes against scikit-learn's implementation. After splitting, the red wine subset contains 1,599 samples, and the white wine subset contains 4,898 samples.\n",
        "\n",
        "For each subset, we apply scikit-learn's train_test_split to divide the data into training and test sets. We then train two models separately on each subset: our own implementation of Gaussian Naive Bayes and scikit-learn's GaussianNB. We compare the two approaches in terms of the estimated means, standard deviations, class priors, joint log likelihoods, and predictions.\n",
        "\n",
        "## Dataset Reference\n",
        "Cortez, P., Cerdeira, A., Almeida, F., Matos, T. & Reis, J. (2009) Wine Quality [Dataset]. UCI Machine Learning Repository. Available at: https://doi.org/10.24432/C56S3T\n",
        " (Accessed: 6 December 2025)."
      ],
      "metadata": {
        "id": "QHNSL_Z6MtFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "def train_models(X_train, y_train, var_smoothing=1e-9):\n",
        "    \"\"\"\n",
        "    Train sklearn's GaussianNB model and our implemented GaussianNaiveBayes model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : ndarray\n",
        "        Training feature matrix.\n",
        "    y_train : ndarray\n",
        "        Training labels.\n",
        "    var_smoothing : float, optional (default=1e-9)\n",
        "        The var_smoothing parameter passed into both models.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    g : GaussianNB\n",
        "        The trained sklearn GaussianNB model.\n",
        "    m : GaussianNaiveBayes\n",
        "        The trained custom GaussianNaiveBayes model.\n",
        "    \"\"\"\n",
        "    g = GaussianNB(var_smoothing=var_smoothing)\n",
        "    g.fit(X_train, y_train)\n",
        "\n",
        "    m = GaussianNaiveBayes(var_smoothing=var_smoothing)\n",
        "    m.train(X_train, y_train)\n",
        "\n",
        "    return g, m\n",
        "\n",
        "\n",
        "def check_model(X_train, X_test, y_train, y_test, var_smoothing_list, dataset_name):\n",
        "    \"\"\"\n",
        "    Validate that our implemented Gaussian Naive Bayes matches sklearn's GaussianNB.\n",
        "\n",
        "    This function checks:\n",
        "        - class means\n",
        "        - variances\n",
        "        - class priors\n",
        "        - joint log likelihoods\n",
        "        - predictions\n",
        "        - accuracies\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train, X_test, y_train, y_test : ndarray\n",
        "        Training and testing feature matrices and labels.\n",
        "    var_smoothing_list : list of floats\n",
        "        List of var_smoothing values to test.\n",
        "    dataset_name : str\n",
        "        Name of the dataset (e.g., \"red wine\", \"white wine\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Testing GaussianNB implementation on the {dataset_name} dataset\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # First test var_smoothing = default 1e-9\n",
        "    gnb_sklearn, imp_model = train_models(X_train, y_train)\n",
        "\n",
        "    # Check mean\n",
        "    assert np.allclose(imp_model.mu, gnb_sklearn.theta_, rtol=1e-9, atol=1e-8), \\\n",
        "        \"The means (mu) differ from sklearn.\"\n",
        "\n",
        "    # Check class priors\n",
        "    assert np.allclose(imp_model.label_priors, gnb_sklearn.class_prior_, rtol=1e-9, atol=1e-8), \\\n",
        "        \"The class priors differ from sklearn.\"\n",
        "\n",
        "    acc_sklearn_9 = gnb_sklearn.score(X_test, y_test)\n",
        "    acc_imp_9 = imp_model.accuracy(X_test, y_test)\n",
        "\n",
        "    print(\"Initial checks passed for var_smoothing = 1e-9\")\n",
        "\n",
        "    # Loop over all var_smoothing values\n",
        "    for v in var_smoothing_list:\n",
        "\n",
        "        gnb_sklearn, imp_model = train_models(X_train, y_train, v)\n",
        "\n",
        "        # Check variance\n",
        "        assert np.allclose(imp_model.var, gnb_sklearn.var_, rtol=1e-9, atol=1e-8), \\\n",
        "            f\"Variances differ when var_smoothing = {v}\"\n",
        "\n",
        "        # Check joint log-likelihood\n",
        "        assert np.allclose(\n",
        "            imp_model._joint_log_likelihood(X_test),\n",
        "            gnb_sklearn._joint_log_likelihood(X_test),\n",
        "            rtol=1e-9,\n",
        "            atol=1e-8\n",
        "        ), f\"Joint log likelihoods differ when var_smoothing = {v}\"\n",
        "\n",
        "        # Check predictions\n",
        "        y_pred_sklearn = gnb_sklearn.predict(X_test)\n",
        "        y_pred_imp = imp_model.predict(X_test)\n",
        "\n",
        "        assert np.all(y_pred_sklearn == y_pred_imp), \\\n",
        "            f\"Predictions differ when var_smoothing = {v}\"\n",
        "\n",
        "        # Check accuracy\n",
        "        acc_sklearn = gnb_sklearn.score(X_test, y_test)\n",
        "        acc_imp = imp_model.accuracy(X_test, y_test)\n",
        "\n",
        "        assert np.allclose(acc_sklearn, acc_imp, rtol=1e-9, atol=1e-8), \\\n",
        "            f\"Accuracies differ when var_smoothing = {v}\"\n",
        "\n",
        "        print(f\"Passed checks for var_smoothing = {v}\")\n",
        "\n",
        "    print(\"\\nAccuracy comparison for var_smoothing = 1e-9:\")\n",
        "    print(f\"  sklearn GaussianNB accuracy     : {acc_sklearn_9:.4f}\")\n",
        "    print(f\"  implemented GaussianNB accuracy : {acc_imp_9:.4f}\")\n",
        "\n",
        "    print(\"\\nAll checks passed for the dataset:\", dataset_name)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "Ht9Htl9SbSfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_smoothing_list = [0, 1e-8, 1e-9, 1e-10, 1e-12]\n",
        "\n",
        "# checking the white wine set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.3, random_state=2025, stratify=y_white)\n",
        "\n",
        "check_model(X_train, X_test, y_train, y_test, var_smoothing_list, \"White Wine\")"
      ],
      "metadata": {
        "id": "6byaV_siULBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff8e3d5-72de-422f-8b80-24fd77bae4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing GaussianNB implementation on the White Wine dataset\n",
            "================================================================================\n",
            "\n",
            "Initial checks passed for var_smoothing = 1e-9\n",
            "Passed checks for var_smoothing = 0\n",
            "Passed checks for var_smoothing = 1e-08\n",
            "Passed checks for var_smoothing = 1e-09\n",
            "Passed checks for var_smoothing = 1e-10\n",
            "Passed checks for var_smoothing = 1e-12\n",
            "\n",
            "Accuracy comparison for var_smoothing = 1e-9:\n",
            "  sklearn GaussianNB accuracy     : 0.4585\n",
            "  implemented GaussianNB accuracy : 0.4585\n",
            "\n",
            "All checks passed for the dataset: White Wine\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the red wine set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red, test_size=0.3, random_state=2025, stratify=y_red)\n",
        "\n",
        "check_model(X_train, X_test, y_train, y_test, var_smoothing_list, \"Red Wine\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_G_eKs45OxM",
        "outputId": "00c93cc3-0cd4-4640-866e-9bd79709d82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing GaussianNB implementation on the Red Wine dataset\n",
            "================================================================================\n",
            "\n",
            "Initial checks passed for var_smoothing = 1e-9\n",
            "Passed checks for var_smoothing = 0\n",
            "Passed checks for var_smoothing = 1e-08\n",
            "Passed checks for var_smoothing = 1e-09\n",
            "Passed checks for var_smoothing = 1e-10\n",
            "Passed checks for var_smoothing = 1e-12\n",
            "\n",
            "Accuracy comparison for var_smoothing = 1e-9:\n",
            "  sklearn GaussianNB accuracy     : 0.5687\n",
            "  implemented GaussianNB accuracy : 0.5687\n",
            "\n",
            "All checks passed for the dataset: Red Wine\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = \\\n",
        "  train_test_split(X_red, y_red, \\\n",
        "  test_size=0.3, random_state=2025, stratify=y_red)\n",
        "\n",
        "g = GaussianNB()\n",
        "g.fit(X_train, y_train)\n",
        "\n",
        "m = GaussianNaiveBayes()\n",
        "m.train(X_train, y_train)\n",
        "\n",
        "print(g.theta_ - m.mu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1SLDIi3pSh0",
        "outputId": "61b5d731-776f-4367-ec86-5740f9961040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(g.var_ - m.var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylD9O4glpWqy",
        "outputId": "4dbfdc52-f2c8-4d76-aaa9-4ef5bc77ea4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff = g._joint_log_likelihood(X_test) - m._joint_log_likelihood(X_test)\n",
        "\n",
        "print(np.allclose(diff, 0, atol=1e-12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XMUmUXbpcXL",
        "outputId": "bf41fd00-b754-4cc5-e119-b14aceafef90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}